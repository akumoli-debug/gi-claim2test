{"paper_id": "1", "title": "Memory Retention Is Not Enough to Master Memory Tasks in RL", "year": 2025, "claims": [{"claim_text": "Temporal Range remains small in fully observed control tasks, indicating minimal memory dependence when full state information is available.", "intervention": "Temporal Range metric", "capability": "accurate measurement of memory dependence", "conditions": "in fully observed control environments"}, {"claim_text": "Temporal Range scales with the task's ground-truth lag in Copy-k tasks, demonstrating the metric accurately captures known memory requirements.", "intervention": "Temporal Range metric", "capability": "accurate quantification of memory use", "conditions": "in Copy-k diagnostic tasks with known ground-truth memory lags"}, {"claim_text": "Temporal Range aligns with the minimum history window required for near-optimal return as confirmed by window ablations.", "intervention": "Temporal Range metric", "capability": "prediction of sufficient context window size", "conditions": "across diagnostic and control tasks when validated against window ablation experiments"}, {"claim_text": "The LEM-based proxy enables computation of Temporal Range when the original policy blocks gradients or is black-box, by training a compact Long Expressive Memory policy on the same task.", "intervention": "Long Expressive Memory (LEM) proxy policy", "capability": "memory dependence measurement for non-differentiable policies", "conditions": "when the original policy is black-box or blocks gradients"}]}
{"paper_id": "2", "title": "Real-Time Recurrent Reinforcement Learning", "year": 2025, "claims": [{"claim_text": "RTRRL using biologically plausible gradient computation methods (RFLO or diagonal RTRL) combined with TD(\u03bb) is capable of solving a diverse set of partially observable reinforcement learning tasks.", "intervention": "RTRRL with RFLO/diagonal RTRL and TD(\u03bb)", "capability": "partial observability", "conditions": "in partially observable Markov decision processes (POMDPs)"}, {"claim_text": "The same Meta-RL RNN architecture when trained using BPTT achieves similar accuracy to RTRRL, but with worse time complexity.", "intervention": "RTRRL", "capability": "computational efficiency", "conditions": "compared to BPTT-trained architectures with similar accuracy"}, {"claim_text": "RNNs can compensate for partial observability in POMDPs by aggregating information about the sequence of observations, unlike linear functions which cannot infer hidden state variables.", "intervention": "recurrent neural networks", "capability": "partial observability", "conditions": "in partially observable Markov decision processes where hidden state inference is required"}, {"claim_text": "The use of feedback alignment does not diminish performance in many cases compared to standard weight transport methods.", "intervention": "feedback alignment", "capability": "learning performance", "conditions": "in neural network training scenarios"}, {"claim_text": "RTRRL succeeds in learning a policy from one continuing stream of experience alone without requiring batched experience replay or multi-step unrolling.", "intervention": "RTRRL online learning", "capability": "sample efficiency and online learning", "conditions": "with continuous experience streams without replay buffers"}]}
{"paper_id": "3", "title": "Toward Task Generalization via Memory Augmentation in Meta-RL", "year": 2025, "claims": [{"claim_text": "TAVT significantly enhances generalization to OOD tasks compared to existing meta-RL methods across various MuJoCo and Meta-World environments.", "intervention": "Task-Aware Virtual Training (TAVT)", "capability": "out-of-distribution task generalization", "conditions": "across various MuJoCo and Meta-World environments"}, {"claim_text": "TAVT accurately aligns task latents for both training tasks and OOD test tasks, reflecting task characteristics more effectively than PEARL, CCM, VariBad, and LDM.", "intervention": "TAVT with metric-based representation learning", "capability": "task representation accuracy", "conditions": "for both training and out-of-distribution test tasks"}, {"claim_text": "CCM improves task representation learning by integrating contrastive learning with PEARL, where task latents from the same task are trained to be close while latents from different tasks are trained to be distant.", "intervention": "contrastive learning integration (CCM)", "capability": "task distinction and representation learning", "conditions": "in context-based meta-RL settings"}, {"claim_text": "LDM improves OOD generalization by training policies with virtual tasks created via linear interpolation of task latents learned by VariBad.", "intervention": "virtual tasks via linear interpolation (LDM)", "capability": "out-of-distribution generalization", "conditions": "using task latents from VariBad"}, {"claim_text": "PEARL achieves sample-efficient learning and faster convergence compared to traditional methods by learning task representations from off-policy samples.", "intervention": "PEARL's off-policy task representation learning", "capability": "sample efficiency and convergence speed", "conditions": "compared to traditional meta-RL methods"}]}
{"paper_id": "4", "title": "Task-Aware Virtual Training for OOD Generalization", "year": 2025, "claims": [{"claim_text": "By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Recent works have begun to address this by bridging disjoint spaces to en- hance generalization (Team, 2025).", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "3) We achieve state-of-the-art performance on over 800 tasks in the Minecraft environment, demonstrat- ing that adaptive action-space selection yields su- perior generalization and robustness compared to static baselines.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": ", 2025) that agents mastering distinct action spaces exhibit strong generalization capa- bilities and enhanced performance within specific domains.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Does the ability to autonomously and dynamically select the most appropriate action space yield superior performance compared to static baselines? Q3: Generalization and Robustness.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Does the model maintain its generalization capabilities on unseen tasks after being fine-tuned via multi- turn RL on a limited set of training tasks? 4.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "RL Facilitates Robust OOD Generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "This confirms that reinforcement learning, when applied atop large pre-trained VLA models, serves as a critical component for achieving out- of-distribution (OOD) generalization in scalable embodied agents.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Quantita- tive results in Table 1 further confirm that the STRL-enhanced CrossAgent achieves consistently higher success rates across a wider range of tasks, demonstrating superior cross-task generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Generalization Evaluation While the results in Table 1 demonstrate the im- pressive OOD performance of CrossAgent, a criti- cal question remains regarding the trade-off be- tween reinforcement learning and generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "To investigate this rigor- ously, we compare the agent\u2019s performance on In-Distribution (ID) training tasks versus OOD evaluation tasks, as detailed in Table 2.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Robust Performance on OOD Tasks.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "These results indicate that CrossAgent possesses superior generalization ca- pabilities compared to other RL baselines.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "This significant gap confirms that the STRL stage is instrumental in preventing the policy from col- lapsing into sub-optimal modes, thereby facili- tating better generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "In contrast, single- 9  Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning Table 2 | Evaluation results of RL agents on In-Distribution (ID) and Out-of-Distribution (OOD) tasks.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Method In-Distribution Evaluation Out-of-Distribution Evaluation Mine Blocks Kill Entities Craft Items All Tasks Mine Blocks Kill Entities Craft Items All Tasks RawHA-RL 65.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "They struggle considerably with OOD tasks out- side their specialized domains, suggesting that restricting the action space limits the model\u2019s abil- ity to transfer learned skills to novel scenarios.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Analysis of the Generalization Gap (ID vs.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "OOD).", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Comparing ID and OOD performance reveals distinct behaviors: Baselines such as RawHA-RL and MotionHA-RL achieve very high success rates on ID tasks (e.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "However, their sharp per- formance drop on OOD tasks highlights a ten- dency toward overfitting.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "8% overall), maintains a much nar- rower generalization gap.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "CrossAgent sets a new standard for OOD generalization, establishing that a unified agent capable of dynamic interface switching is more robust than specialized experts.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "In the interaction phase, Grounding Ac- tions are engaged to accurately target the specific wood block, ensuring successful harvesting.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Since these steps are invariant across tasks, Raw Actions dominate this phase, leveraging their precision and stability to execute rote \"muscle memory\" sequences efficiently.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "These results validate that treating action-space selection as a learnable component\u2014rather than a static, hand- crafted constraint\u2014significantly enhances both performance and generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Jarvis- 1: Open-world multi-task agents with memory- augmented multimodal language models.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Challenges for Reinforcement Learning Minecraft presents several unique challenges that test the robustness and generalization capabilities of RL agents: High Information Density.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Task Variety and Generalization.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "For instance, the motor skills required to chop a tree can be adapted to combat, while the logic used for crafting a wooden pickaxe serves as a foundational step for crafting diamond tools.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": ", a diamond sword) requires a long sequence of prerequisite actions: mining wood, crafting a crafting table, mining stone, smelting iron, and finding diamonds.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "By decoupling the semantic intent of an action from its spatial execution parameters, ground- ing actions significantly improve generalization across diverse visual scenarios (Cai et al.", "intervention": null, "capability": "generalization", "conditions": null}]}
{"paper_id": "5", "title": "MemRL: Self-Evolving Agents via Runtime RL on Episodic Memory", "year": 2025, "claims": [{"claim_text": "MemRL MEMRL: SELF-EVOLVING AGENTS VIA RUNTIME REINFORCEMENT LEARNING ON EPISODIC MEMORY Shengtao Zhang1,\u2217, Jiaqian Wang2,* , Ruiwen Zhou3 , Junwei Liao1,4 , Yuchen Feng5 , Weinan Zhang1,4 , Ying Wen1,4 , Zhiyu Li5 , Feiyu Xiong5 , Yutao Qi2 , Bo Tang6,5,\u2020, Muning Wen1,\u2020 1Shanghai Jiao Tong University, 2Xidian University, 3National University of Singapore, 4Shanghai Innovation Institute, 5MemTensor (Shanghai) Technology Co.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "While Large Lan- guage Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory- based methods rely on passive semantic matching that often retrieves noise.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "To address these challenges, we propose MEMRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "MEMRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "0 MemRL (Ours) MemP RAG No Memory Figure 1: Benchmark Runtime Learning performance of MEMRL.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We compare MEMRL against state-of-the-art memory baselines (MemP) and standard retrieval methods (RAG).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The hallmark of human intelligence lies in the delicate balance between the stability of cognitive reasoning and the plasticity of episodic memory (Grossberg, 2013; McClelland et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "To address this challenge, inspired by the human cognitive mechanism of constructive simulation, we propose MEMRL, a framework that facilitates self-evolving agents by explicitly decoupling the model\u2019s stable cognitive reasoning from dynamic episodic memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Drawing on value-iteration approaches in Reinforcement Learning (RL) to estimate expected experience utilities (Sutton & Barto, 2018), we formalize the interaction between the frozen LLM and external memory as a Markov Decision Process (MDP) (Puterman, 2014).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Unlike traditional methods that optimize the backbone model, MEMRL optimizes the policy of memory usage to maximize expected utility.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "MEMRL organizes memory into a structured Intent-Experience-Utility triplet.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "In summary, our contributions are threefold: \u2022 We propose a runtime learning framework based on Model-Memory decoupling and the Intent- Experience-Utility triplet, which reconciles the stability-plasticity dilemma by enabling agents to learn without parameter updates.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "\u2022 We introduce MEMRL, a non-parametric reinforcement learning algorithm that implements Value- Aware Retrieval and Utility-Driven Memory Curation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "This allows agents to self-evolve by optimizing memory utility, establishing a new paradigm for enhancing agent capabilities.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "In contrast, our method frames memory usage as a learnable decision problem and applies non-parametric reinforcement learning on memory to bypass the risk.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "3 AGENTIC MEMORY To bypass the costs of fine-tuning, external memory systems have evolved from a static RAG paradigm to dynamic, governable memory structures (Lewis et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Early agentic memory introduced reflection mechanisms and hierarchical management to handle long context 3  MemRL Intent A m1 m2 Retrieved Generation Outcome \ud835\udc2b\ud835\udc1e\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1d Failure m2 m3 Retrieved Generation Outcome \ud835\udc2b\ud835\udc1e\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1d Success m1 Intent Experience Utility Memory \u2133\ud835\udc61 Intent Experience Utility m2 m3 Intent Experience Utility m1 Intent Experience Utility Memory \u2133\ud835\udc61+1 m2 Intent Experience Utility m3 Intent Experience Utility m4 Intent Experience Utility m5 Intent Experience Utility m1 m5 Retrieved Generation Outcome \ud835\udc2b\ud835\udc1e\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1d Success m1 m4 Retrieved Generation Outcome \ud835\udc2d Time Step \ud835\udc2d+ \ud835\udfcf Time Step \ud835\udc2d+ \ud835\udfd0 Time Step Intent B   \ud835\udc2b\ud835\udc1e\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1d Failure Intent A Intent A Figure 3: An illustrative example of memory-augmented decision making under a Markov Decision Process.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "At time step t, the agent starts with an initial memory set Mt.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "In contrast, another intent (Intent B) succeeds and its associated experience is added to memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "This example shows how memory retrieval enables knowledge reuse across intents, implicitly supporting transfer across tasks through shared experiences.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "More recent frameworks have systematized the memory lifecycle, focusing on unified storage and structured indexing for complex tasks (Li et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "However, except for training additional learnable modules, most existing methods still rely predominantly on semantic similarity or heuristic rules, lacking a rigorous metric to evaluate the actual utility of a memory in maximizing returns.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Inspired by cognitive theories of memory reconsolidation (Schacter & Addis, 2007; Gick & Holyoak, 1980; Nader et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "3 PROBLEM FORMULATION In this section, we formally define the problem of memory-augmented generation and establish the theoretical link between agent policy and memory retrieval.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We adopt the formulation of Memory-Based Markov Decision Process (M-MDP) (Zhou et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Figure 3 provides an illustrative example of this memory-augmented decision process, showing how retrieval outcomes and memory evolution unfold over multiple time steps.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "1 MEMORY-AUGMENTED AGENT POLICY To enable the agent to self-evolve, we inherit the M-MDP as the problem formulation (Zhou et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The M-MDP is formally defined as a tuple \u27e8S, A, P, R, \u03b3, M\u27e9, where S denotes the state space, A the action space, P : S \u00d7 A \u2192R the transition dynamics, R : S \u00d7 A \u2192R the reward function, \u03b3 \u2208[0, 1) the discount factor, and M = (S \u00d7 A \u00d7 R)\u2217the evolving memory space containing past experiences (Zhou et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "In this setting, the policy \u03c0 not only generates tokens 4  MemRL but first selects a memory context m based on a retrieval distribution \u00b5(\u00b7|s, M), allowing the agent to leverage historical data for better performance in downstream tasks.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": ", a user query or task description, and has access to an external memory bank M.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Following this formulation, the behavior of a memory-augmented agent can be decomposed into two distinct phases: Retrieve and Generation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The joint policy \u03c0(yt|st, Mt) is defined as the marginal probability over all possible retrieved memory items (Zhou et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": ", 2025): \u03c0(yt|st, Mt) = X m\u2208Mt \u00b5(m|st, Mt) | {z } Retrieval Policy \u00b7 pLLM(yt|st, m) | {z } Inference Policy (1) where: \u2022 \u00b5(m|st, Mt) represents the Retrieval Policy, which assigns a probability to selecting a specific memory context m composed of past intents and experiences.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "from the memory bank Mt given the current state st.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "It models the likelihood of generating output yt conditioned on both the query st and the retrieved context m.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "In previous RAG or memory-based agentic paradigms, the retrieval policy \u00b5 is usually determined by a fixed vector similarity metric, e.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "While effective for semantic matching, such policies fail to account for the utility of a memory, i.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "2 NON-PARAMETRIC REINFORCEMENT LEARNING To overcome the limitations of static similarity-based retrieval, we operationalize the M-MDP framework by formulating memory retrieval as a value-based decision-making process.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Unlike parametric approaches that optimize \u03c0LLM via weight updates, we aim to optimize the retrieval policy \u00b5(m|s, M) directly within the memory space.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Consequently, the action space At becomes dynamic and discrete, corresponding to the selection of a specific m from the current memory bank Mt.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Under this formulation, retrieving from memory is no longer a passive matching task but an active action at = m taken to augment the generator (Zhou et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": ", execution success, we can update the Q-value of the retrieved memory context using a Temporal-Difference (TD) error (Sutton, 1988): Q(s, m) \u2190Q(s, m) + \u03b1[r + \u03b3 max Q(s\u2032, m\u2032) \u2212Q(s, m)] (3) where \u03b1 is the learning rate.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "By maintaining and updating these Q-values explicitly within the memory structure, MEMRL provides a non-parametric learning manner with a theoretical guarantee that enables the agent to self-evolve its capabilities through interaction.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "(Top) The end-to-end learning loop: given a query s, the agent retrieves context mctx from memory M, generates output y, and updates memory value Q based on reward R.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "(Bottom Right) Utility Update: Memory values (Q) are updated using environmental rewards to distinguish functional utility from semantic similarity.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Instead of modifying the model weights \u03b8, MEMRL optimizes the retrieval policy \u00b5(m|s, M) within an evolving memory space.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "As illustrated in Figure 4, the framework consists of three core components: (i) a structured Intent-Experience-Utility memory bank, (ii) a Two-Phase Retrieval mechanism that decouples semantic recall from value-aware selection, and (iii) a Runtime Utility Update rule that stabilizes Q-value estimation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "1 MEMORY STRUCTURE: THE INTENT-EXPERIENCE-UTILITY TRIPLET To support value-based decision-making, we structure the external memory M not merely as key- value pairs, but as a set of triplets: M = {(zi, ei, Qi)}|M| i=1 , (4) where zi represents the Intent Embedding (e.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "2 TWO-PHASE RETRIEVAL: FROM SEMANTIC RECALL TO VALUE-AWARE SELECTION Standard RAG or memory systems rely solely on semantic similarity, implicitly assuming that \u201csimilar implies useful.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We 6  MemRL compute the cosine similarity sim(s, zi) and filter the memory bank: C(s) = TopKk1({i|sim(s, zi) > \u03b4}, by sim) (5) where \u03b4 is a sparsity threshold.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "This phase acts as a coarse filter, reducing the search space from the entire memory M to a relevant subset C(s).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Notably, if C(s) = \u2205, MEMRL injects no memory and relies solely on the frozen LLM for broader exploration.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "3 RUNTIME LEARNING: NON-PARAMETRIC RL ON MEMORY The core of MEMRL is the continuous refinement of Q-values based on environmental feedback, enabling the agent to \u201cremember\u201d what works.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "During runtime, MEMRL performs learning entirely in memory space.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Meanwhile, for each sampled trajectory, we use an LLM to summarize the experience, and write it back into the memory bank as a new triplet (z(s), enew, Qinit), enabling continual expansion of experience while keeping the LLM parameters unchanged.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "8 implements a form of memory reconsolidation (Haubrich & Nader, 2016): once a memory is retrieved and applied, its utility is reinforced or atten- uated according to subsequent outcomes.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Together, these components realize a stability\u2013plasticity balance: the frozen LLM preserves stable cognitive reasoning, while the evolving memory utilities provide the plastic channel for continual adaptation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "5 STABILITY ANALYSIS We analyze the stability of MEMRL from a reinforcement learning perspective, focusing on the convergence behavior of the utility estimates stored in memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We show that under mild and realistic assumptions, the learned utility values converge in expectation to stable estimates of memory effectiveness, with bounded variance.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "At each time step t, the agent observes an intent state st, retrieves a memory item mt \u2208Mt, generates an output yt, and receives a scalar reward rt \u2208[\u22121, 1] indicating task success or failure.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "For each retrieved memory, MEMRL updates its utility using the exponential moving average rule as formulated in Eq.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "For clarity in this analysis, we consider a fixed state\u2013memory pair (s, m) and write Qt \u2261Qt(s, m).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "These assumptions guarantee that the learning target is well-defined: the expected reward for any specific task-memory pair is time-invariant.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "(12) Thus, constant-step-size updates do not induce unbounded oscillations; instead, they yield stable utility estimates that track expected memory effectiveness while filtering high-frequency noise.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The stability of the local estimate (Theorem 1) extends to the global memory utility Q(m).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "BigCodeBench Lifelong Agent Bench ALFWorld HLE Method Code Gen (Last / CSR) OS Task (Last / CSR) DB Task (Last / CSR) Exploration (Last / CSR) Knowledge Frontier (Last / CSR) Model GPT-4o GPT-4o-mini GPT-4o-mini GPT-4o-mini Gemini-3-pro No Memory 0.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "where S(m) \u225c{s \u2208S|sim(s, zm) \u2265\u03c4A} denotes the effective support set for memory m, compris- ing all task intents s sufficiently similar to the memory\u2019s intent embedding zm to satisfy the Phase-A retrieval criterion.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We evaluate MEMRL against a comprehensive suite of memory-augmented baselines under a unified frozen-backbone setting to isolate the contribution of memory mechanisms: (i) RAG-based Approaches: RAG (Lewis et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "(ii) Agentic Memory: Mem0 (Chhikara et al.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": ", 2025), which introduce structured write/read operations or procedural memory distillation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We evaluate our MEMRL and baselines under two distinct settings: Runtime Learning, which assesses the ability to learn and adapt within a training session, and Transferring, which evaluates the generalization capability of the learned memory on unseen tasks.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We compare our MEMRL against various retrieval and memory baselines using best validation results.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "BigCodeBench Lifelong Agent Bench ALFWorld Method Code Generation (Acc) OS Task (Acc) DB Task (Acc) Exploration (Acc) Model GPT-4o GPT-4o-mini GPT-4o-mini GPT-4o-mini No Memory 0.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "324) and 82% over the No Memory baseline (0.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "We evaluate memory transferability by freezing the memory bank after training and testing on held-out sets (30% split).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "As shown in Table 2, MEMRL exhibits superior generalization compared to baseline methods.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "These results validate that the Value-Aware Retrieval and Non-Parametric RL mechanism in MEMRL does not merely overfit to training instances; instead, it filters out low-value memories, retaining high-utility experiences that facilitate generalization to unseen tasks.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "1 EFFECTIVENESS OF RUNTIME RL To isolate the effect of runtime RL, we compare the memory mechanisms with and without the RL component (i.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "This confirms that semantic similarity must act as an anchor for retrieval, while RL provides the necessary gradient for optimizing memory selection.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "To investigate the impact of memory capacity on reasoning performance, we conducted an ablation study on a subset of the HLE benchmark (Computer Science/AI category).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "In sequential tasks, a retrieved memory must be valid for the entire trajectory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "By propagating the final reward backward to the memory utility Q, MEMRL effectively learns to verify the whole trajectory, filtering out brittle policies that look correct only on the surface.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "1% in the highest, indicating that a major driver of performance is the Critic\u2019s ability to rank memories by their likelihood of leading to successful task completion.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Further analysis of memory composition (Figure 8b) suggests a secondary source of gain: robustness.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Case study: a high-Q \u201cfailure\u201d memory as a transferable near-miss.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "For example, one failure memory with Q = 0.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "When retrieved in later episodes, this single \u201cfailure\u201d memory supports perfect downstream outcomes in our logs (15/15 successes), demonstrating that the memory is valuable precisely because it captures a fixable near-miss rather than an uninformative failure.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "0 Q-Estimate Range 0 20 40 60 80 100 Memory Proportion (%) 22% 78% 47% 53% 73% 27% 79% 21% 82% 18% 84% 16% 86% 14% 88% 12% Pool Avg: 74.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "6% Memory Composition by Q-Value SUCCESS Memory FAILURE Memory (b) Memory Composition Figure 8: Q-Value Analysis.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "250 Performance Gain (after - before) BigCodeBench LifeLongBench-OS LifeLongBench-DB AlfWorld HLE Linear fit Scaled diagonal Figure 11: Similarity-based Generalization nearest neighbors leading to repeated failures, MEMRL allows the agent to identify and reinforce non-obvious but effective strategies.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "4 IMPACT OF TASK SIMILARITY ON MEMORY EFFICACY.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "This confirms that for highly repetitive procedural tasks, memory serves as an effective shortcut to optimal trajectories.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The HLE Anomaly: Generalization vs.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "This distinction highlights MEMRL\u2019s versatility: it supports both pattern generalization in structured domains and specific knowledge acquisition in diverse domains.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "6 CONCLUSION In this paper, we introduced MEMRL, a novel framework that enables LLMs to self-evolve through non-parametric reinforcement learning on episodic memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Addressing the limitations of semantic retrieval and the instability of parameter fine-tuning, MEMRL treats memory retrieval as a value- based decision process.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "By structuring memory into Intent-Experience-Utility triplets and applying Bellman updates, the agent learns to differentiate high-value strategies from semantic noise without modifying the backbone model weights.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Our extensive evaluations across diverse domains\u2014ranging from code generation to embodied naviga- tion\u2014demonstrate that MEMRL significantly outperforms existing memory-augmented baselines in both runtime learning and generalization to unseen tasks.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Theoretical and empirical analyses further reveal that MEMRL effectively resolves the stability-plasticity dilemma: the frozen LLM provides robust reasoning, while the evolving memory utility acts as a plastic channel for adaptation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Mem0: Building production-ready ai agents with scalable long-term memory, 2025.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Maximum likelihood from incomplete data via the em algorithm.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Memp: Exploring agent procedural memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Deconstructing episodic memory with construction.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Memory reconsolidation.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Behavioral neuroscience of learning and memory, pp.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Licomemory: Lightweight and cognitive agentic memory for efficient long-term reasoning.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Online real-time recurrent learning using sparse connections and selective learning.", "intervention": "recurrent policy", "capability": "memory", "conditions": null}, {"claim_text": "Retrieval feedback memory enhancement large model retrieval generation method.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Memos: An operating system for memory-augmented generation (mag) in large language models.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Gradient episodic memory for continual learning.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G.", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "Meminsight: Autonomous memory augmentation for llm agents.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The future of memory: remembering, imagining, and the brain.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Evo-memory: Benchmarking llm agent test-time learning with self-evolving memory.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "A-mem: Agentic memory for llm agents.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Task memory engine: Spatial memory for robust multi-step llm agents.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Learn to memorize: Optimizing llm-based agents with adaptive memory framework.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "19  MemRL A THEORETICAL ANALYSIS AND PROOFS In this section, we provide the detailed derivation for the convergence of the Q-value estimation under the Exponential Moving Average (EMA) update rule, and extend the analysis to the global stability of memory utility under task distributions.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "1 PROOF OF THEOREM 1: CONVERGENCE OF EMA ESTIMATION We aim to prove that for a fixed task-memory pair (s, m) with a stationary reward distribution, the Q-value estimate Qt(s, m) converges in expectation to the true mean reward \u03b2(s, m).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "While tasks within a memory cluster S(m) \u225c{s|sim(s, zm) > \u03c4A} may vary, the Smoothness Assumption implies their rewards are drawn from a distribution with bounded variance \u03c32 S(m).", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "The derived bound \u03b1 2\u2212\u03b1\u03c32 S(m) guarantees that the memory utility Q(m) will not diverge but will instead oscillate within a controlled range around the true expected utility.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "1 THE CONVERGENCE OBJECTIVE Our ultimate goal is to ensure that the estimated utility Q(m) converges to the true expected return of memory m.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}, {"claim_text": "Trust Region: It constrains the policy to the support set S, preventing the agent from retrieving high-Q but semantically irrelevant memories (out-of-distribution errors).", "intervention": null, "capability": "generalization", "conditions": null}, {"claim_text": "With a fixed data distribution, the standard Bellman contraction property ensures that Qt(m) converges to the unique fixed point: lim t\u2192\u221eQt(m) \u2192E\u00b5\u2217[r|m] (20) Thus, our framework theoretically guarantees that the memory values converge to the true expected returns under the optimal retrieval policy.", "intervention": "memory mechanism", "capability": "memory", "conditions": null}]}
